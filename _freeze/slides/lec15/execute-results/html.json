{
  "hash": "739e2f1d52c1775fdcd75dd94afc2a8b",
  "result": {
    "markdown": "---\ntitle: \"EM algorithm\"\nauthor: \"Dr. Alexander Fisher\"\nexecute:\n  warning: true\nformat: \n    revealjs:\n      smaller: true\n---\n\n\n## Announcements\n\n-   Reminder: [project](/project.html) announced before spring break.\n\n-   Exam 2 March 31st.\n\n# Expectation-maximization\n\n-   Check out the original article by Dempster, Laird and Rubin [here](https://www.jstor.org/stable/2984875).\n\n> Dempster, Arthur P., Nan M. Laird, and Donald B. Rubin. \"Maximum likelihood from incomplete data via the EM algorithm.\" Journal of the royal statistical society: series B (methodological) 39.1 (1977): 1-22.\n\n## Intro to EM\n\n-   Expectation-maximization \"EM\" is a special case of the MM algorithm.\n\n-   Fundamentally, we still wish to maximize an objective function $f$ and we need to come up with a surrogate $g$ that minorizes $f$.\n\n-   the common example we'll re-visit is maximum likelihood estimation\n\n. . . \n\n:::callout-note\n- In the EM literature, the surrogate is often labeled \"Q\". We will use \"Q\" as well.\n\n- In these slides we will write the log-likelihood, $\\log f(y | \\theta)$ as $L(\\theta)$ for convenient notation.\n:::\n\n## EM MLE\n\n-   Let $f(y | \\theta)$ be the likelihood of observed data $y$ given some vector of parameters $\\theta$.\n\n-   Let $h(x|\\theta)$ be the likelihood of the **complete data** $x$ given $\\theta$. Typically $y \\subset x$. Under this paradigm, some data $z = x \\setminus y$ are missing. Data may be missing in the ordinary sense of dropped observations or in an abstract sense that makes estimation easier.\n\n. . .\n\n**Surrogate**\n\n\n$$\nQ(\\theta | \\theta_n) = \\mathbb{E} [\\log h(x | \\theta) \\ | \\ y, \\theta_n]\n$$\n\n\nminorizes the log-likelihood $L(\\theta) = \\log f(y | \\theta)$.\n\n. . .\n\n::: callout-note\nThe surrogate is defined as an expected alue.\n:::\n\nThe algorithm proceed by iterating two steps:\n\n1.  compute the **expectation** to find the surrogate function $Q(\\theta | \\theta_n)$.\n\n2.  **maximize** the surrogate.\n\n## Information inequality (Gibb's inequality)\n\nTo motivate the surrogate function, we'll need one additional inequality in our toolkit.\n\nSuppose $p(x)$ and $q(x)$ are probability densities.\n\n\n$$\nD_{KL}(p || q)  = \\int \np(x) \n\\ln \\frac{p(x)}{q(x)} \ndx\n\\geq 0\n$$\n\n\n. . .\n\nSaid another way,\n\n\n$$\n\\int p(x) \\ln p(x) dx - \\int p(x) \\ln q(x) dx \\geq 0.\n$$\n\n\n. . .\n\nSaid yet another way,\n\n\n$$\n\\mathbb{E_p}\\left[\n\\ln p(x)\n\\right] \n\\geq\n\\mathbb{E_p}\\left[\n\\ln q(x)\n\\right] \n$$\n\n\n## Verifying the surrogate\n\n#### Claim\n\n\n$$\nQ(\\theta | \\theta_n) = \\mathbb{E} [\\log h(x | \\theta) \\ | \\ y, \\theta_n]\n$$\n\n\nminorizes the log-likelihood $L(\\theta) = \\log f(y | \\theta)$.\n\n#### Proof\n\n. . .\n\n1. Assume the complete data $x = (y, z)$, where $y$ is observed and $z$ is missing.\n\n\n$$\nf(y|\\theta) = \\frac{h(y,z|\\theta)}{g(z|y, \\theta)}\n$$\n\nby Bayes' theorem.\n\n. . . \n\n2. Log both sides and take the expectation using current iterate $\\theta_n$ to parameterize the density of the missing data.\n\n\n$$\n\\underbrace{\\log f(y|\\theta)}_{L(\\theta)} = \\underbrace{\\int g(z|y, \\theta_n) \\log h(y,z |\\theta) dz}_{Q(\\theta|\\theta_n)} - \\underbrace{\\int g(z|y, \\theta_n) \\log g(z|y, \\theta) dz}_{R(\\theta|\\theta_n)}\n$$\n\n\n. . . \n\nSince true $\\forall \\  \\theta$, also true for $\\theta_n$,\n\n\n$$\nL(\\theta_n) = {Q(\\theta_n|\\theta_n)} - {R(\\theta_n|\\theta_n)}\n$$\n\n\n## Verification continued\n\n3. Form the difference $L(\\theta) - L(\\theta_n)$.\n\n\n$$\nL(\\theta) - L(\\theta_n) = {Q(\\theta|\\theta_n)} - {Q(\\theta_n|\\theta_n)} \n\\underbrace{- \\left[{R(\\theta|\\theta_n)} - {R(\\theta_n|\\theta_n)}\\right]}_{\n\\geq~0 \\text{ by Gibb's inequality}\n}\n$$\n\n*notice* the negative sign is included here.\n\n. . . \n\n4. Drop the positive term for inequality.\n\n\n$$\nL(\\theta) - L(\\theta_n) \\geq {Q(\\theta|\\theta_n)} - {Q(\\theta_n|\\theta_n)} \n$$\n\n\nand re-arrange\n\n\n$$\nL(\\theta) \\geq {Q(\\theta|\\theta_n)} - {Q(\\theta_n|\\theta_n)} + L(\\theta_n)\n$$\n\n\nto see that the log-likelihood dominates the surrogate up to an irrelevant constant.\n\n# Example\n\n## Censored data\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n| Miles at which breaks fail (thousands) |\n|:--------------------------------------:|\n|                100.000                 |\n|                 37.044                 |\n|                 7.254                  |\n|                 40.211                 |\n:::\n:::\n\n\nLet $x$ be the number of miles (in thousands) at which a car's breaks fail. Measurements stop after 100 thousand miles, thus the data is right-censored.\n\nWe model the lifetime of a car's breaks as an exponential random variable with unknown rate of failure $\\lambda$.\n\n. . . \n\nLet $N$ be the total number of observations. The complete data contains both observed and unobserved lifetimes. $N = N_o + N_c$ where $N_o$ is the number of break failures actually observed ($x < 100$) and $N_c$ is the number of censored data points.\n\n#### Complete data likelihood\n\n\n$$\n\\prod_{i = 1}^{N} \\lambda e^{-\\lambda x_i}\n$$\n\n## The model\n\n#### Complete data log-likelihood\n\n\n$$\nL(x |\\lambda) = N \\log \\lambda - \\lambda \\sum_{i = 1}^N x_i \n$$\n\n\nSplitting the fully observed vs censored data,\n\n\n$$\nL(x |\\lambda) = N \\log \\lambda - \\lambda \\left( \n\\sum_{i = 1}^{N_o} y_i + \\sum_{j = 1}^{N_c} z_j\n\\right)\n$$\n\nwhere we've split the $x$ into the observed $y$ and unobserved $z$.\n\n## Surrogate\n\n\n$$\n\\begin{aligned}\nQ(\\lambda | \\lambda_n) &=\n\\mathbb{E}\\left[\nL(x |\\lambda) \\ | \\ y, \\lambda_n\n\\right]\\\\\n&=\nN \\log \\lambda - \\lambda \\left(\\sum_{i = 1}^{N_o} y_i - \n\\sum_{j = 1}^{N_c} \\underbrace{\\mathbb{E} [z_j]}_{\\text{w.r.t. } z_j |\\lambda_n}\n\\right)\n\\end{aligned}\n$$\n\n\n. . . \n\nNote: $z_j = x_j | x_j > c$ where $c = 100$ is the censoring cut-off. Therefore this is an expectation of a [truncated](https://en.wikipedia.org/wiki/Truncated_distribution) exponential.\n\n. . .\n\n- E step:\n\n\n$$\nQ(\\lambda | \\lambda_n) = N \\log \\lambda - \\lambda \\left(\\sum_{i = 1}^{N_o} y_i - \n\\sum_{j = 1}^{N_c} \\left(\nc + \\frac{1}{\\lambda_n} \\right)\n\\right)\n$$\n\n. . .\n\n- M step:\n\n$\\frac{dQ}{d\\lambda} = 0 \\implies$\n\n\n$$\n\\lambda_{n+1} = \\frac{N}{\\sum_{i = 1}^{N_o} y_i + N_c \\left( c + \\frac{1}{\\lambda_n}\n\\right)}\n$$\n\n\n## Exercise\n\nCode the EM algorithm on the previous slide for the simulated data-set of break failures below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5)\nN = 250 # number of observations\ntrueLambda = 0.01\nxcomplete = rexp(N, rate = trueLambda) %>% round(digits = 3)\nc = 100 # censor time\n\nx = xcomplete\nx[x >= 100] = 100\n```\n:::\n\n\nNote: you only observe `x`. \n\n. . . \n\nSolution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndownload.file(\"https://sta323-sp23.github.io/scripts/EM-example.R\",\n              destfile = \"EM-example.R\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}