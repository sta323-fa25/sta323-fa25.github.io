{
  "hash": "d457fb3d41ebdb0f94e005baaf6db008",
  "result": {
    "markdown": "---\ntitle: \"Newton-Raphson\"\nauthor: \"Dr. Alexander Fisher\"\nexecute:\n  warning: true\nformat: \n    revealjs:\n      smaller: true\n---\n\n\n# Overview\n\n## univariate Newton-Raphson\n\n-   If $a$ satisfies $f(a) = 0$, $a$ is said to be a \"fixed point\" or \"root\" of the function\n-   Newton-Raphson is a \"root-finding\" method\n-   Based on first order approximation of a function, $\\ f(a) \\approx f(x) + f'(x)(a - x)$\n\nNear fixed points, the first order approximation is: \n\n\n$$\n0 \\approx f(x) + f'(x)(a - x).\n$$\n\n\nWe are trying to find $a$, so if $f'(x)$ invertible, we rearrange\n\n\n$$\na = x - \\frac{f(x)}{f'(x)}\n$$\n\n\n. . .\n\nand the procedural update is\n\n\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n$$\n\n\n## Code example\n\n\n::: {.cell}\n\n:::\n\n\nIn practice, we tolerate close solutions, i.e. $a$ such that $f(a) \\approx 0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewton = function(f, fp, x, tol) {\n  for (i in 1:100) {\n    x = x - f(x) / fp(x)\n    if (abs(f(x)) < tol) {\n      return(x)\n    }\n  }\n  return(x)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nf = function(x) {\n  return(x ^ 3 - 5 * x + 1)\n}\n\nfp = function(x) {\n  return(3 * (x ^ 2) - 5.0)\n}\nnewton(f, fp, -2.0, 1e-14)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.330059\n```\n:::\n:::\n\n\n- `f` is the function we are finding a root of.\n\n\n## Where we start matters\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec12_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## What can go wrong\n\n- derivative not continuous at the root\n\n- If $f'(x_n)=0$, can't proceed\n\n- iterates can \"cycle\" rather than converge to the solution\n\n- If $f'(x_n)$ is small, but not zero, approximate solutions can diverge to infinity.\n\n- Sensitive to initial point $x_0$\n\n# Example: math\n\n## Beta-binomial\n\n\n$$\nX \\sim Binomial(n, p)\n$$\n\n$$\np \\sim Beta(\\alpha, \\beta)\n$$\n\n\nThis data-generative process models a set of $N$ binomial experiments, each possibly having their own local $p_i$, and local $n_i$ for $i \\in \\{1, \\ldots N\\}$.\n\n. . . \n\nExample: \n\n> Schuckers, ME\n          \"[Using the Beta-binomial distribution to assess performance of a biometric identification   device](http://myslu.stlawu.edu/~msch/biometrics/papers.htm)\"\n           International Journal of Image and Graphics, vol. 3, no. 3, July 2003\n\n. . .\n\nData:\n\n\n::: {.cell}\n::: {.cell-output-display}\n| id|  n| success|\n|--:|--:|-------:|\n|  1|  9|       3|\n|  2|  9|       0|\n|  3|  8|       8|\n|  4|  6|       6|\n|  5|  9|       8|\n:::\n:::\n\n\n## Beta-binomial\n\n\n\n$$ \nf_x(\\mathbf{x}|\\mathbf{p}, \\mathbf{n}) = \n\\prod_{i = 1}^{N} {n_i \\choose{x_i}} p_i^{x_i}(1-p_i)^{n_i-x_i}  \n$$\n\nwhere $\\mathbf{x} = \\{x_i \\}$, $\\mathbf{p} = \\{ p_i \\}$ and $\\mathbf{n} = \\{n_i \\}$.\n\n\n$$\nf_p(\\mathbf{p} | \\alpha, \\beta) = \\prod_{i = 1}^{N} \\frac{p_i^{\\alpha - 1} (1-p_i)^{\\beta - 1}}{B(\\alpha, \\beta)}\n$$\n\n\nwhere $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$. \n\n:::callout-note\n- $\\Gamma$ is the `gamma` function (read `?gamma` and [wikipedia](https://en.wikipedia.org/wiki/Gamma_function)).\n- `beta()` is the R function for $B(\\alpha, \\beta)$.\n:::\n\n. . . \n\nFundamentally, we are interested in the underlying Beta distribution that the $p_i$ are drawn from. We don't care about the individual $p_i$ themselves. We integrate out $\\mathbf{p}$,\n\n\n$$\n\\begin{aligned}\nf_x(\\mathbf{x}|\\mathbf{n}, \\alpha, \\beta) &= \\int f_x(\\mathbf{x}, \\mathbf{p} | \\mathbf{n}, \\alpha, \\beta)\n\\ d\\mathbf{p}\\\\\n&=\n\\int\nf_x(\\mathbf{x}|\\mathbf{p}, \\mathbf{n}) f_p(\\mathbf{p} | \\alpha, \\beta)\n\\ d\\mathbf{p}\n\\end{aligned}\n$$\n\n\n## Integrating out the nuisance parameters continued\n\n\n$$\n\\begin{aligned}\n\\int\nf_x(\\mathbf{x}|\\mathbf{p}, \\mathbf{n}) f_p(\\mathbf{p} | \\alpha, \\beta)\n\\ d\\mathbf{p} &=\n\\int\n\\prod_{i = 1}^{N} {n_i \\choose{x_i}}\n\\frac{p_i^{x_i + \\alpha - 1} (1-p_i)^{n_i - x_i +\\beta - 1}}{B(\\alpha, \\beta)}\n\\end{aligned}\nd \\mathbf{p}\n$$\n\n\nConjugacy! This is proportional to a Beta distribution, hence\n\n\n$$\n\\begin{aligned}\nf_x(\\mathbf{x}|\\mathbf{n}, \\alpha, \\beta) &=\n\\prod_{i = 1}^N {n_i \\choose x_i} \n\\frac{B(x_i + \\alpha, n_i - x_i +\\beta)}{B(\\alpha, \\beta)}\\\\\n&= \\prod_{i = 1}^N {n_i \\choose x_i} \n\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\n\\frac{\\Gamma(x_i + \\alpha)\\Gamma(n_i -x_i + \\beta)}{\\Gamma(n_i + \\alpha + \\beta)}\n\\end{aligned}\n$$\n\n\n. . . \n\nThis in fact our likelihood! We can still simplify by pulling out the part that isn't indexed by $i$,\n\n\n$$\nL(\\alpha, \\beta) = \n\\left(\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\\right)^N \\cdot\n\\prod_{i = 1}^N {n_i \\choose x_i} \n\\frac{\\Gamma(x_i + \\alpha)\\Gamma(n_i -x_i + \\beta)}{\\Gamma(n_i + \\alpha + \\beta)}\n$$\n\n\n\n## Log-likelihood \n\n\n$$\n\\log L = N \\left(\\log \\Gamma(\\alpha+\\beta) + \\log \\Gamma(\\alpha) -\\log\\Gamma(\\beta) \\right)+\\\\\n\\sum_{i=1}^N \\log {n_i \\choose x_i} + \\log \\Gamma(x_i + \\alpha) +\n\\log \\Gamma(n_i -x_i + \\beta) - \\log {\\Gamma(n_i + \\alpha + \\beta)}\n$$\n\n. . . \n\nWe want to maximize the log-likelihood.\n\n. . .  \n\n### Gradient of log-likelihood\n\n- derivative of log-Gamma function is the [digamma(https://en.wikipedia.org/wiki/Digamma_function) function], often denoted by the Greek $\\psi$ (\"psi\").\n\n\n$$\n\\frac{\\partial}{\\partial \\alpha} \\log L = N \\left[\\psi(\\alpha + \\beta) + \\psi(\\alpha)\\right] +\n\\sum_{i=1}^N \\psi(x_i + \\alpha) - \\psi(n_i + \\alpha + \\beta)\n$$\n\n$$\n\\frac{\\partial}{\\partial \\beta}\\log L = \nN \\left[\\psi(\\alpha + \\beta) + \\psi(\\beta)\\right] + \\sum_{i=1}^N\n\\psi(n_i - x_i + \\beta) - \\psi(n_i + \\alpha + \\beta)\n$$\n\n## Jacobian\n\n- second derivative of log-Gamma function is the [trigamma function](https://en.wikipedia.org/wiki/Trigamma_function), often denoted by the Greek $\\psi_1$ (\"psi\") with a subscript \"1\".\n\n\n$$\n\\frac{\\partial^2}{\\partial \\alpha^2} \\log L = N \\left[\\psi_1(\\alpha + \\beta) + \\psi_1(\\alpha)\\right] +\n\\sum_{i=1}^N \\psi_1(x_i + \\alpha) - \\psi_1(n_i + \\alpha + \\beta)\n$$\n\n$$\n\\frac{\\partial^2}{\\partial \\beta^2}\\log L = \nN \\left[\\psi_1(\\alpha + \\beta) + \\psi_1(\\beta)\\right] + \\sum_{i=1}^N\n\\psi_1(n_i - x_i + \\beta) - \\psi_1(n_i + \\alpha + \\beta)\n$$\n\n- Same thing but with trigamma functions!\n\n. . . \n\n\n$$\n\\frac{\\partial^2}{\\partial \\alpha \\partial \\beta}\\log L = \nN \\left[\\psi_1(\\alpha + \\beta)\\right] - \\sum_{i=1}^N \\psi_1(n_i + \\alpha + \\beta)\n$$\n\n\nIn general, the Jacobian,\n\n\n$$\nJ=\\left[\\begin{array}{ccc}\n\\dfrac{\\partial f_{1}(\\mathbf{x})}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial f_{1}(\\mathbf{x})}{\\partial x_{n}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{\\partial f_{m}(\\mathbf{x})}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial f_{m}(\\mathbf{x})}{\\partial x_{n}}\n\\end{array}\\right]\n$$\n\n\n# Example: code\n\n## multivariate Newton-Raphson\n\nThe same intuition applies, but in higher dimensions.\n\nLet $\\mathbf{a}$ satisfy $f(\\mathbf{a}) = 0$.\n-   Based on first order approximation of a function, $\\ f(\\mathbf{a}) \\approx f(\\mathbf{x}) + f'(\\mathbf{x})(\\mathbf{a}- \\mathbf{x})$\n\nNear fixed points, the first order approximation is: \n\n\n$$\n0 \\approx f(\\mathbf{x}) + Df(\\mathbf{x})(\\mathbf{a} - \\mathbf{x}).\n$$\n\n\nTo find the root, we rearrange (solve for $\\mathbf{a}$) again and iterate\n\n\n$$\n\\mathbf{x}_{n+1} = \\mathbf{x}_n - J^{-1}(\\mathbf{x}_n) f(\\mathbf{x}_n) \n$$\n\n## A good starting point\n\nMethod of moments. \n\nSet the moments equal to the sample mean, $\\bar{x}$ and sample variance $s^2$.\n\n\n$$\n\\hat{\\alpha} = \\frac{n m_1 - m_2}{n (\\frac{m_2}{m_1} - m_1 - 1) + m_1} \n$$\n\n$$\n\\hat{\\beta} = \\frac{(n- m_1) (n - \\frac{m_2}{m_1})}{\nn (\\frac{m_2}{m_1} - m_1 - 1 + m_1)\n}\n$$\n\n## Exercise: find the MLE\n\n- Code Newton-Raphson to find $\\hat{\\alpha}_{MLE}$ and $\\hat{\\beta}_{MLE}$ for the 19th century hospital record example [here](https://en.wikipedia.org/wiki/Beta-binomial_distribution).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = data.frame(families = c(3, 24, 104, 286, 670, 1033, 1343, 1112,\n                            829, 478, 181, 45, 7),\n               m = seq(0, 12),\n               n = rep(12, 13)) %>%\n  mutate(p = m / n)\n```\n:::\n",
    "supporting": [
      "lec12_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}